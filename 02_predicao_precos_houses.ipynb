{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_predicao_precos_houses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXXuhCmcWWWe"
      },
      "source": [
        "\n",
        "# Previsão de preços de casas:um exemplo de regressão\n",
        "\n",
        "Este bloco de notas contém os exemplos de código encontrados no Capítulo 3, Seção 6 do Deep Learning with Python. Observe que o texto original apresenta muito mais conteúdo, em particular mais explicações e figuras: neste bloco de notas, você encontrará apenas o código-fonte e comentários relacionados.\n",
        "\n",
        "Em nossos dois exemplos anteriores, estávamos considerando problemas de classificação, em que o objetivo era prever um único rótulo discreto de um ponto de dados de entrada. Outro tipo comum de problema de aprendizado de máquina é a \"regressão\", que consiste em prever um valor contínuo em vez de um rótulo discreto. Por exemplo, prever a temperatura amanhã, dados dados meteorológicos, ou prever o tempo que um projeto de software levará para ser concluído, dadas suas especificações.\n",
        "\n",
        "Não misture \"regressão\" com o algoritmo \"regressão logística\": confusamente, \"regressão logística\" não é um algoritmo de regressão, é um algoritmo de classificação.\n",
        "\n",
        "\n",
        "**O conjunto de dados do Boston Housing Price**\n",
        "Estaremos tentando prever o preço médio das casas em um determinado subúrbio de Boston em meados da década de 1970, dados alguns pontos de dados sobre o subúrbio na época, como a taxa de criminalidade, a taxa de imposto sobre a propriedade local, etc.\n",
        "\n",
        "O conjunto de dados que usaremos tem outra diferença interessante de nossos dois exemplos anteriores: ele tem muito poucos pontos de dados, apenas 506 no total, dividido entre 404 amostras de treinamento e 102 amostras de teste, e cada \"recurso\" nos dados de entrada (por exemplo, taxa de criminalidade é uma característica) tem uma escala diferente. Por exemplo, alguns valores são proporções, que assumem valores entre 0 e 1, outros assumem valores entre 1 e 12, outros entre 0 e 100 ...\n",
        "\n",
        "Vamos dar uma olhada nos dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dnKW5CIVvy0",
        "outputId": "f893e535-acfe-49b7-fc07-9864e0287323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3CUp98mXQMh",
        "outputId": "0ffe4736-1779-4396-dc3b-6827c29da3d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwEgtLHUWZes",
        "outputId": "e854e655-d0f0-45c9-c9b9-9b0c94c4a60e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkJfbZYgXveZ"
      },
      "source": [
        "Como você pode ver, temos 404 amostras de treinamento e 102 amostras de teste. Os dados são compostos por 13 recursos. Os 13 recursos nos dados de entrada são os seguintes:\n",
        "\n",
        "*   Taxa de criminalidade per capita.\n",
        "*   Proporção de terrenos residenciais zoneados para lotes com mais de 25.000 pés quadrados.\n",
        "*   Proporção de acres de negócios não varejistas por cidade.\n",
        "*   Variável dummy de Charles River (= 1 se a área limita o rio; 0 caso contrário).\n",
        "*   Número médio de cômodos por moradia.\n",
        "*   Proporção de unidades ocupadas pelo proprietário construídas antes de 1940.\n",
        "*  Distâncias ponderadas para cinco centros de empregos de Boston.\n",
        "*  Índice de acessibilidade às rodovias radiais.\n",
        "*  Taxa de imposto de propriedade de valor total por $ 10.000.\n",
        "*  Proporção aluno-professor por cidade.\n",
        "*  1000 * (Bk - 0,63) ** 2 onde Bk é a proporção de negros por cidade.\n",
        "*  % de status inferior da população.\n",
        "\n",
        "As metas são os valores medianos das casas ocupadas pelo proprietário, em milhares de dólares:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gdpoXM7XSQZ",
        "outputId": "950ab543-c247-4581-e6b5-e1452930bde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_targets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
              "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
              "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
              "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
              "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
              "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
              "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
              "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
              "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
              "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
              "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
              "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
              "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
              "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
              "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
              "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
              "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
              "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
              "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
              "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
              "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
              "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
              "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
              "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
              "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
              "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
              "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
              "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
              "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
              "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
              "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
              "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
              "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
              "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
              "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
              "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
              "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDo3hi8OX0gn"
      },
      "source": [
        "\n",
        "# Preparando os dados\n",
        "\n",
        "Seria problemático alimentar os valores de uma rede neural em que todos assumissem intervalos totalmente diferentes. A rede pode ser capaz de se adaptar automaticamente a esses dados heterogêneos, mas certamente tornaria o aprendizado mais difícil. Uma prática recomendada amplamente difundida para lidar com esses dados é fazer a normalização por recurso: para cada recurso nos dados de entrada (uma coluna na matriz de dados de entrada), vamos subtrair a média do recurso e dividir pelo desvio padrão, que o recurso será centralizado em torno de 0 e terá um desvio padrão da unidade. Isso é feito facilmente no Numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqjlYw0qXyYw"
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1fYLAdBX4QX"
      },
      "source": [
        "Observe que as quantidades que usamos para normalizar os dados de teste foram calculadas usando os dados de treinamento. Nunca devemos usar em nosso fluxo de trabalho qualquer quantidade calculada nos dados de teste, mesmo para algo tão simples como a normalização de dados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBqmV-vAX4p3"
      },
      "source": [
        "**Construindo nossa rede**\n",
        "Como há poucas amostras disponíveis, usaremos uma rede muito pequena com duas camadas ocultas, cada uma com 64 unidades. Em geral, quanto menos dados de treinamento você tiver, pior será o overfitting, e usar uma rede pequena é uma forma de mitigar o overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWLHI8YwX56-"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "    # Porque precisaremos instanciar\n",
        "    # o mesmo modelo várias vezes,\n",
        "    # usamos uma função para construí-lo.\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu',\n",
        "                           input_shape=(train_data.shape[1],)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoPJQtmtZTlk"
      },
      "source": [
        "Nossa rede termina com uma única unidade, e nenhuma ativação (ou seja, será uma camada linear). Esta é uma configuração típica para regressão escalar (ou seja, regressão em que tentamos prever um único valor contínuo). A aplicação de uma função de ativação restringiria o intervalo que a saída pode assumir; por exemplo, se aplicássemos uma função de ativação sigmóide à nossa última camada, a rede só poderia aprender a prever valores entre 0 e 1. Aqui, como a última camada é puramente linear, a rede é livre para aprender a prever valores em qualquer faixa.\n",
        "\n",
        "Observe que estamos compilando a rede com a função de perda mse - Erro Quadrático Médio, o quadrado da diferença entre as previsões e as metas, uma função de perda amplamente usada para problemas de regressão.\n",
        "\n",
        "Também estamos monitorando uma nova métrica durante o treinamento: mae. Isso significa erro médio absoluto. É simplesmente o valor absoluto da diferença entre as previsões e os alvos. Por exemplo, um MAE de 0,5 neste problema significaria que nossas previsões estão erradas em \\ $ 500 em média."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODS5WPgbZVh0"
      },
      "source": [
        "Validando nossa abordagem usando validação K-fold\n",
        "Para avaliar nossa rede enquanto ajustamos seus parâmetros (como o número de épocas usadas para o treinamento), poderíamos simplesmente dividir os dados em um conjunto de treinamento e um conjunto de validação, como estávamos fazendo em nossos exemplos anteriores. No entanto, como temos tão poucos pontos de dados, o conjunto de validação acabaria sendo muito pequeno (por exemplo, cerca de 100 exemplos). Uma consequência é que nossas pontuações de validação podem mudar muito dependendo de quais pontos de dados escolhemos usar para validação e quais escolhemos para treinamento, ou seja, as pontuações de validação podem ter uma alta variação em relação à divisão de validação. Isso nos impediria de avaliar de forma confiável nosso modelo.\n",
        "\n",
        "A melhor prática em tais situações é usar a validação cruzada K-fold. Consiste em dividir os dados disponíveis em K partições (normalmente K = 4 ou 5), instanciar K modelos idênticos e treinar cada um nas partições K-1 enquanto avalia a partição restante. A pontuação de validação para o modelo usado seria então a média das pontuações de validação K obtidas.\n",
        "\n",
        "Em termos de código, isso é simples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQJnNIcyZVLc",
        "outputId": "9dcc3c87-203c-4410-a164-2b67348f2dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare os dados de validação: dados da partição # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare os dados de treinamento: dados de todas as outras partições\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Construir o modelo Keras (já compilado)\n",
        "    model = build_model()\n",
        "    # Treine o modelo (no modo silencioso, verboso = 0)\n",
        "    model.fit(partial_train_data, partial_train_targets,\n",
        "              epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    # Avalie o modelo nos dados de validação\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "    all_scores.append(val_mae)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lOVvVa3Zab0",
        "outputId": "b613619b-970f-4512-fa32-6250259790c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_scores"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9392619132995605, 2.5057201385498047, 2.751328229904175, 2.487156391143799]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_luh-JPZb2c",
        "outputId": "249d806e-7612-4ca7-9e4d-d2c8a2ac154d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.mean(all_scores)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4208666682243347"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Wu2yTAZexM"
      },
      "source": [
        "Como você pode notar, as diferentes execuções realmente mostram pontuações de validação bastante diferentes, de 2.1 a 2.9. Sua média (2,4) é uma métrica muito mais confiável do que qualquer uma dessas pontuações - esse é o ponto principal da validação cruzada K-fold. Neste caso, perdemos em média \\ $ 2.400, o que ainda é significativo considerando que os preços variam de \\ $ 10.000 a \\ $ 50.000.\n",
        "\n",
        "\n",
        "Vamos tentar treinar a rede um pouco mais: 500 épocas. Para manter um registro do desempenho do modelo em cada época, modificaremos nosso loop de treinamento para salvar o log de pontuação de validação por época:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbE3G0exZdMM"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Alguma limpeza de memória\n",
        "K.clear_session()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLWV7NGYZgVU",
        "outputId": "b0fb5412-112d-4ff0-a061-a7fd86f521fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare os dados de validação: dados da partição # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare os dados de treinamento: dados de todas as outras partições\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Construir o modelo Keras (já compilado)\n",
        "    model = build_model()\n",
        "    # Treine o modelo (no modo silencioso, verboso = 0)\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    mae_history = history.history['val_mae']\n",
        "    all_mae_histories.append(mae_history)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLIyjofMcsiO"
      },
      "source": [
        "\n",
        "Podemos então calcular a média das pontuações MAE por época para todas as dobras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akc-BX05ZiCc"
      },
      "source": [
        "average_mae_history = [\n",
        "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSK8QsIacq_x"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmPZDTqZcwRk"
      },
      "source": [
        "\n",
        "Pode ser um pouco difícil ver o gráfico devido a problemas de escala e variação relativamente alta. Vamos:\n",
        "\n",
        "Omita os primeiros 10 pontos de dados, que estão em uma escala diferente do resto da curva.\n",
        "Substitua cada ponto por uma média móvel exponencial dos pontos anteriores, para obter uma curva suave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwhQx3h4cu5E"
      },
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
        "\n",
        "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ6PAfI4czYU"
      },
      "source": [
        "\n",
        "De acordo com este gráfico, parece que a validação MAE para de melhorar significativamente após 80 épocas. Passado esse ponto, começamos a overfitting.\n",
        "\n",
        "Assim que terminarmos de ajustar outros parâmetros do nosso modelo (além do número de épocas, também podemos ajustar o tamanho das camadas ocultas), podemos treinar um modelo de \"produção\" final em todos os dados de treinamento, com os melhores parâmetros, em seguida, observe seu desempenho nos dados de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrvDDEpkc0BM"
      },
      "source": [
        "# Obtenha um novo modelo compilado.\n",
        "model = build_model()\n",
        "# Treine-o com todos os dados.\n",
        "model.fit(train_data, train_targets,\n",
        "          epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yox8T1cc18V"
      },
      "source": [
        "test_mae_score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}